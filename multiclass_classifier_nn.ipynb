{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 10 Digit Classifier With 2 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given Template Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset Class for any dataset.\n",
    "    This is a python class object, it inherits functions from \n",
    "    the pytorch Dataset object.\n",
    "    For anyone unfamiliar with the python class object, see \n",
    "    https://www.w3schools.com/python/python_classes.asp\n",
    "    or a more complicated but more detailed tutorial\n",
    "    https://docs.python.org/3/tutorial/classes.html\n",
    "    For anyone familiar with python class, but unfamiliar with pytorch\n",
    "    Dataset object, see \n",
    "    https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, anno_csv) -> object:\n",
    "        self.anno_data = pd.read_csv(anno_csv)\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anno_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_name = self.anno_data.iloc[idx, 0]\n",
    "        data_location = self.data_dir + data_name\n",
    "        data = np.float32(np.load(data_location))\n",
    "        # print(\"read data:\", data) # TODO: Remove\n",
    "        # This is for one-hot encoding of the output label\n",
    "        gt_y = np.float32(np.zeros(10))\n",
    "        index = self.anno_data.iloc[idx, 1]\n",
    "        gt_y[index] = 1\n",
    "        return data, gt_y\n",
    "    \n",
    "    def split_to_batches(self, batch_num):\n",
    "        \"\"\"\n",
    "        Function takes in batch number and returns a tensor of batch_num samples\n",
    "        \"\"\"\n",
    "\n",
    "        # Shuffle the annotation dataframe\n",
    "        # Source: https://www.aporia.com/resources/how-to/shuffle-dataframe-rows/\n",
    "        shuffled_anno_data = self.anno_data.sample(frac=1)\n",
    "        \n",
    "        # Split the shuffled dataframe into an equally split list\n",
    "        # Source: https://stackoverflow.com/questions/54730276/how-to-randomly-split-a-dataframe-into-several-smaller-dataframes\n",
    "        batches = np.array_split(shuffled_anno_data, batch_num)\n",
    "        return batches\n",
    "\n",
    "    def head(self):\n",
    "        return self.anno_data.head()\n",
    "    \n",
    "    def get_batches(self, batch_num):\n",
    "        #Randomly split your training data into mini-batches where each mini-batch has 50 samples\n",
    "        #Since we have 50000 training samples, and each batch has 50 samples,\n",
    "        #the total number of batch will be 1000\n",
    "        # YOU ARE NOT ALLOWED TO USE DATALOADER CLASS FOR RANDOM BATCH SELECTION\n",
    "\n",
    "        # device = utilsNN.set_torch_device()\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "        # Create a list of indices\n",
    "        indices = np.arange(0, len(self))\n",
    "\n",
    "        # Shuffle the randomized indices\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        # Split the indices array into batches\n",
    "        batched_indices = np.array_split(indices, batch_num)\n",
    "\n",
    "        # print(batched_indices)\n",
    "        # print(\"batched_indices shape:\", len(batched_indices), \"x\", len(batched_indices[0]))\n",
    "\n",
    "        # Batch the data from the randomized indices and return them\n",
    "        batched_data = []\n",
    "        train_batched_Y = []\n",
    "        for indices in batched_indices:\n",
    "            # For each batch\n",
    "            data_batch, gt_y_batch = My_dataset.get_data(self, indices)\n",
    "            batched_data.append(data_batch)\n",
    "            train_batched_Y.append(gt_y_batch)\n",
    "\n",
    "        data = torch.as_tensor(np.array(batched_data), device=device)\n",
    "        gt_y = torch.as_tensor(np.array(train_batched_Y), device=device)\n",
    "        # gt_y = train_batched_Y\n",
    "\n",
    "        # t_list = [t0, t1, t2]\n",
    "        # torch.stack(t_list, dim =0 )\n",
    "\n",
    "        # print(\"batched data device:\", data.device) # TODO: Remove\n",
    "        \n",
    "        return data, gt_y    \n",
    "\n",
    "    @staticmethod\n",
    "    def get_data(dataset: Dataset, indices):\n",
    "        \"\"\"\n",
    "        Returns data and gt_y as lists\n",
    "        \"\"\"\n",
    "        device = utilsNN.set_torch_device('cpu')\n",
    "        \n",
    "        # print(\"Indices:\", indices) # Todo: remove\n",
    "        data_list = []\n",
    "        gt_y_list = []\n",
    "        for index in indices:\n",
    "            datum, gt_y_single = dataset[index]\n",
    "            data_list.append(datum)\n",
    "            gt_y_list.append(gt_y_single)\n",
    "        data = torch.as_tensor(np.array(data_list), device=device)\n",
    "        gt_y = torch.as_tensor(np.array(gt_y_list), device=device)\n",
    "\n",
    "        # print(\"data device:\", data.device) # TODO: Remove\n",
    "\n",
    "        return data, gt_y\n",
    "\n",
    "\n",
    "class utilsNN:\n",
    "    @staticmethod\n",
    "    def init_weight(input_dim, output_dim):\n",
    "        # Initializes weights using variant of Xavier weight initialization:\n",
    "        # https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "        uniform_dist_one = torch.as_tensor(np.random.rand(input_dim+1, output_dim), dtype=torch.float32)\n",
    "        weight = torch.div(2*(uniform_dist_one - 0.5), math.sqrt(input_dim))\n",
    "        # print(\"weight device:\", weight.device) # TODO: Remove\n",
    "        return weight\n",
    "    \n",
    "    @staticmethod\n",
    "    def ReLU(layer):\n",
    "        \"\"\"\n",
    "        ReLU Function applied to each layer\n",
    "        \"\"\"\n",
    "        # Create zero matrix\n",
    "        zeros_matrix = torch.zeros_like(layer)\n",
    "\n",
    "        # Perform and return ReLU operation\n",
    "        return torch.maximum(layer, zeros_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward_step(layer, weights, activation_func):\n",
    "        # device = utilsNN.set_torch_device()\n",
    "        X = utilsNN.cat_ones(layer)\n",
    "        # print(f\"Multiplying layer ({X.shape}) and weights ({weights.shape})\") # TODO: Remove\n",
    "        # print(f\"X device: {X.device}, weights device: {weights.device}\") # TODO: Remove\n",
    "        z = torch.matmul(X, weights.to(X.device))\n",
    "        output = activation_func(z)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(tensor):\n",
    "        \"\"\"\n",
    "        Applies softmax to entire tensor\n",
    "        \"\"\"\n",
    "        # device = utilsNN.set_torch_device()\n",
    "        # Subtract max for stable softmax (https://stackoverflow.com/questions/42599498/numerically-stable-softmax)\n",
    "        X = tensor - torch.max(input=tensor, dim=tensor.dim()-1, keepdim=True)[0]\n",
    "        \n",
    "        # Now perform softmax function\n",
    "        result = torch.exp(X)\n",
    "        result /= torch.sum(result, dim=tensor.dim()-1, keepdim=True)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_NLL(predicted, actual):\n",
    "        \"\"\"\n",
    "        Negative log likelihood loss function.\n",
    "        \"\"\"\n",
    "        # print(f\"Multiplying actual ({actual.shape}) by expected ({expected.shape})\") # TODO: Remove\n",
    "        M = actual.shape[0]\n",
    "        loss = torch.sum( -torch.multiply(actual, torch.log(predicted)) / M)\n",
    "        return loss.item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def cat_ones(my_tensor):\n",
    "        # device = utilsNN.set_torch_device()\n",
    "        # print(\"my tensor shape:\", my_tensor.shape) # TODO: Remove\n",
    "        \n",
    "        # Create the ones matrix to append\n",
    "        ones_shape_list = list(my_tensor.shape)\n",
    "        ones_shape_list[-1] = 1\n",
    "        ones_shape = tuple(ones_shape_list)\n",
    "        ones_column = torch.ones(ones_shape)\n",
    "        # print(ones_column)\n",
    "        # print(torch.ones((2,3,4)))\n",
    "        concatenated_tensor = torch.cat((my_tensor, ones_column), dim=my_tensor.dim()-1)\n",
    "        # print(concatenated_tensor.shape) # TODO: Remove\n",
    "        return concatenated_tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_output_grad(predicted, actual):\n",
    "        return -torch.div(actual, predicted)\n",
    "            \n",
    "    @staticmethod\n",
    "    def softmax_deriv(softmax_result):\n",
    "        \"\"\"\n",
    "        Return softmax derivative, referenced formula from\n",
    "        https://stats.stackexchange.com/questions/215521/how-to-find-derivative-of-softmax-function-for-the-purpose-of-gradient-descent\n",
    "        \"\"\"\n",
    "        # print(\"softmax_result.shape\", softmax_result.shape) # TODO: Remove\n",
    "        # squeezed_softmax = softmax_result.squeeze()\n",
    "        # print(\"squeezed_softmax.shape\", squeezed_softmax.shape) # TODO: Remove\n",
    "\n",
    "        # Calculate Diag(y)\n",
    "        diag_y_list = [torch.diag(datum) for datum in softmax_result]\n",
    "        diag_y = torch.stack(diag_y_list)\n",
    "        \n",
    "        # print(\"diag_y.shape:\", diag_y.shape) # TODO: Remove\n",
    "        # print(diag_y) # TODO: Remove\n",
    "        y  = softmax_result.unsqueeze(2) # Becomes M x K x 1\n",
    "        y_T = softmax_result.unsqueeze(1) # Becomes M X 1 X K\n",
    "        # yy_T = torch.matmul(softmax_result, torch.transpose(softmax_result, 0, 1))\n",
    "        yy_T = torch.matmul(y, y_T)\n",
    "        # print(\"yy_T shape:\", yy_T.shape) # TODO: Remove\n",
    "        return diag_y - yy_T\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_deriv(relu_result):\n",
    "        # Create a binary tensor to transform\n",
    "        bin_ten = torch.where(relu_result > 0, 1.0, 0.0)\n",
    "\n",
    "        # Create M x N x N identity matrix\n",
    "        identity_m = torch.eye(relu_result.shape[-1])\n",
    "\n",
    "        # Apply the transformation\n",
    "        derivative = torch.einsum(\"bi,ij->bij\", bin_ten, identity_m)\n",
    "\n",
    "        return derivative\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_dzdW(prev_layer, curr_layer_node_num):\n",
    "        \"\"\"\n",
    "        Derivative of z with respect to the weight for layer l, with\n",
    "        the previous layer as an input.\n",
    "\n",
    "        Input must be contain the batches, so dimension of 3\n",
    "        \"\"\"\n",
    "        # print(\"prev_layer_dim:\", prev_layer.dim()) # TODO: Remove\n",
    "        assert(prev_layer.dim() == 2)\n",
    "        identity_m = torch.eye(curr_layer_node_num)\n",
    "        derivative = torch.einsum(\"ij,bn->binj\", identity_m, prev_layer)\n",
    "        # print(f\"Derivative dzdW with shape {derivative.shape}: {derivative}\") # TODO: Remove\n",
    "        return derivative\n",
    "    \n",
    "    @staticmethod \n",
    "    def backward_step(model, layer_index, layer_grad, activation : str, last_pass : bool = False):\n",
    "\n",
    "        # Calculate the derivative of the activation function\n",
    "        if activation == 'softmax':\n",
    "            dg_dz = utilsNN.softmax_deriv(model.layers[layer_index])\n",
    "        elif activation == 'ReLU':\n",
    "            dg_dz = utilsNN.relu_deriv(model.layers[layer_index])\n",
    "        else:\n",
    "            raise Exception(f'Invalid activation function string provided: \"{activation}\"')\n",
    "        \n",
    "        # Calculate the derivative of z, or the layer, with respect to W\n",
    "        dzdW = utilsNN.derivative_dzdW(\n",
    "            prev_layer = model.layers[layer_index-1],\n",
    "            curr_layer_node_num = model.layers[layer_index].shape[1]\n",
    "        )\n",
    "\n",
    "        # Calculate the gradient of the bias\n",
    "        grad_W0 = torch.einsum(\"bij,bj->bi\", dg_dz, layer_grad)\n",
    "\n",
    "        # Calculate the gradient of the weights\n",
    "        grad_W = torch.einsum(\"bijk,bk->bji\", dzdW, grad_W0)\n",
    "        \n",
    "        # Concatenate to form Theta matrix\n",
    "        grad_Theta = torch.cat((grad_W, grad_W0.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # Calculate the gradient of the previous layer if last pass\n",
    "        if not last_pass:\n",
    "            prev_layer_grad = torch.einsum(\"ij,bj->bi\", model.weights[layer_index - 1][:-1].to(device=grad_W0.device), grad_W0)\n",
    "            gradients = (grad_Theta, prev_layer_grad)\n",
    "        else:\n",
    "            gradients = (grad_Theta)\n",
    "\n",
    "        return gradients\n",
    "    \n",
    "    @staticmethod\n",
    "    def update_weights(weights, gradients, learning_rate : float):\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] = weights[i] - learning_rate*(gradients[i].to(device=weights[i].device))\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_acc(prediction, actual):\n",
    "        # Calculate accuracy\n",
    "        accuracy = torch.sum(torch.argmax(prediction, 1) == torch.argmax(actual, 1))/actual.shape[0]\n",
    "        return accuracy.item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_digit_errors(prediction, actual, total_digits):\n",
    "        digit_errors = []\n",
    "        for y in range(total_digits):\n",
    "            indices = torch.nonzero(torch.argmax(prediction, 1) == y, as_tuple=False)\n",
    "            digit_err = 1 - utilsNN.calc_acc(prediction[indices[:,0]], actual[indices[:,0]])\n",
    "            digit_errors.append(digit_err)\n",
    "        \n",
    "        return digit_errors\n",
    "    \n",
    "    @staticmethod\n",
    "    def set_torch_device(forced_device=\"none\"):\n",
    "        if forced_device == \"cpu\" or not torch.cuda.is_available():\n",
    "            device = torch.device('cpu')\n",
    "            torch.set_default_device(device)\n",
    "        elif forced_device == \"cuda\" or torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            torch.set_default_device(device)\n",
    "        return device\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def test(dir : str, anno : str, model = None):\n",
    "\n",
    "\n",
    "\n",
    "class DigitClassifierModelDNN:\n",
    "    def __init__(self, input_dim: int, hidden_layer_num: int, hidden_node_num: int, output_dim: int):\n",
    "        # device = utilsNN.set_torch_device()\n",
    "        \n",
    "        # self.hidden_layer_num = hidden_layer_num\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = [0]*(2 + hidden_layer_num)\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.weights = []\n",
    "\n",
    "        ## Initialize the weight matrix from the input to the first hidden later\n",
    "        Theta_1 = utilsNN.init_weight(input_dim, hidden_node_num) # Includes the bias as the last row\n",
    "        self.weights.append(Theta_1)\n",
    "\n",
    "        ## Initialize the weight matrix for the rest of the hidden layers\n",
    "        for n in range(2, hidden_layer_num + 1):\n",
    "            Theta_n = utilsNN.init_weight(hidden_node_num, hidden_node_num)\n",
    "            # print(\"theta_n type:\", type(Theta_n)) # TODO: Remove\n",
    "            self.weights.append(Theta_n)\n",
    "        \n",
    "        ## Add the final weight for the last hidden layer \n",
    "        self.weights.append(utilsNN.init_weight(hidden_node_num, output_dim))\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        param input is a 2D tensor\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.layers) - 2\n",
    "        self.layers[0] = input\n",
    "\n",
    "        # self.layers[0] = utilsNN.forward_step(input, self.weights[0], utilsNN.ReLU)\n",
    "        for n in range(1, L + 1):\n",
    "            self.layers[n] = utilsNN.forward_step(self.layers[n-1], self.weights[n-1], utilsNN.ReLU)\n",
    "\n",
    "            # print(\"next_hidden_layer:\", hidden_layer_n) # TODO: Remove\n",
    "\n",
    "        self.layers[L+1] = utilsNN.forward_step(self.layers[L], self.weights[L], utilsNN.softmax)\n",
    "\n",
    "        return self.layers[L+1]\n",
    "        \n",
    "    def backward(self, output_grad):\n",
    "        \"\"\"\n",
    "        Propagate backwards\n",
    "        \"\"\"\n",
    "        grad_Theta_3, grad_H_2 = utilsNN.backward_step(\n",
    "            model = self,\n",
    "            layer_index = 3,\n",
    "            layer_grad = output_grad,\n",
    "            activation = \"softmax\",\n",
    "            last_pass = False\n",
    "        )\n",
    "\n",
    "        # TODO: Remove test prints below\n",
    "        # print(f\"grad_Theta_3 with shape {grad_Theta_3.shape}:\", grad_Theta_3)\n",
    "        # print(f\"grad_H_2 with shape {grad_H_2.shape}:\", grad_H_2)\n",
    "\n",
    "        grad_Theta_2, grad_H_1 = utilsNN.backward_step(\n",
    "            model = self,\n",
    "            layer_index = 2,\n",
    "            layer_grad = grad_H_2,\n",
    "            activation = \"ReLU\",\n",
    "            last_pass = False\n",
    "        )\n",
    "\n",
    "        grad_Theta_1 = utilsNN.backward_step(\n",
    "            model = self,\n",
    "            layer_index = 1,\n",
    "            layer_grad = grad_H_1,\n",
    "            activation = \"ReLU\",\n",
    "            last_pass = True\n",
    "        )\n",
    "\n",
    "        # Average the gradients over the batch\n",
    "        grads = [torch.mean(grad_Theta_1, dim=0)]\n",
    "        grads.append(torch.mean(grad_Theta_2, dim=0))\n",
    "        grads.append(torch.mean(grad_Theta_3, dim=0))\n",
    "\n",
    "        gradients = tuple(grads)\n",
    "        return gradients\n",
    "    \n",
    "    def pred(self, input, actual_out):\n",
    "        # Make the prediction\n",
    "        y_hat = self.forward(input)\n",
    "\n",
    "\n",
    "        # # TODO: Remove\n",
    "        # print_tensor(data, \"data\")\n",
    "        # print_tensor(gt_Y, \"gt_Y\")\n",
    "        # print_tensor(pred, \"pred\")\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = utilsNN.calc_acc(prediction=y_hat, actual=actual_out)\n",
    "        digit_errors = utilsNN.calc_digit_errors(prediction=y_hat, actual=actual_out, total_digits=10)\n",
    "        loss = utilsNN.loss_NLL(predicted=y_hat, actual=actual_out)\n",
    "\n",
    "        return (accuracy, digit_errors, loss)\n",
    "    \n",
    "    def save_weights(self, fname : str):\n",
    "        # Convert to desired format\n",
    "        Theta = []\n",
    "        for weight in self.weights:\n",
    "            W = weight.to(device=torch.device('cpu')).numpy()[:-1]\n",
    "            W_0 = weight.to(device=torch.device('cpu')).numpy()[-1]\n",
    "            Theta.append(W)\n",
    "            Theta.append(W_0)\n",
    "        \n",
    "        # Save the file\n",
    "        filehandler = open(fname,\"wb\") \n",
    "        pickle.dump(Theta, filehandler, protocol=2)\n",
    "        filehandler.close()\n",
    "\n",
    "    def load_weights(self, fname : str):\n",
    "        filehandler = open(fname, \"rb\")\n",
    "        Theta = pickle.load(filehandler)\n",
    "        W = Theta[0]\n",
    "        for i in range(1, len(Theta)):\n",
    "            if i % 2 == 0:\n",
    "                W = Theta[i]\n",
    "            else:\n",
    "                weight = np.concatenate((W, np.expand_dims(Theta[i], axis=0)), axis=0)\n",
    "                self.weights[i//2] = torch.as_tensor(weight)\n",
    "        \n",
    "        return self.weights\n",
    "    \n",
    "    def change_weights_device(self, device):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = self.weights[i].to(device = device)\n",
    "                \n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "def PA2_get_data(data_dir, anno_csv, batch_num):\n",
    "    device = utilsNN.set_torch_device('cpu')\n",
    "    # Read the data and labels from the training data\n",
    "    dataset = My_dataset(data_dir=data_dir, anno_csv=anno_csv)\n",
    "    batched_data, batched_labels_enc = dataset.get_batches(batch_num=batch_num)\n",
    "    return batched_data, batched_labels_enc\n",
    "\n",
    "# Main Functions\n",
    "def PA2_train(train_batched_X, train_batched_Y, test_batched_X, test_batched_Y):\n",
    "    # Specifying the device to GPU/CPU. Here, GPU means 'cuda' and CPU means 'cpu'\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    device = utilsNN.set_torch_device()\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate = 0.13\n",
    "\n",
    "    # Initialize the model\n",
    "    model = DigitClassifierModelDNN(\n",
    "        input_dim = train_batched_X.shape[-1],\n",
    "        hidden_layer_num = 2,\n",
    "        hidden_node_num = 100,\n",
    "        output_dim = 10\n",
    "    )\n",
    "    \n",
    "    # Initialize list for losses and accuracies\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    train_errors_digits = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    test_errors_digits = []\n",
    "    \n",
    "\n",
    "    #You can set up your own maximum epoch. You may need  5 or 10 epochs to have a correct model.\n",
    "    my_max_epoch = 10\n",
    "    epochs = np.arange(0, my_max_epoch)\n",
    "    for epoch in epochs:\n",
    "        total_batch = len(train_batched_X)\n",
    "        \n",
    "        # print(len(batched_X[0][0])) # TODO: Remove\n",
    "\n",
    "        print(f\"----Epoch {epoch:>d}----\")\n",
    "\n",
    "        for b in range(total_batch):\n",
    "            '''Compute the loss for each batch, gradient of loss with respect to W, \n",
    "            and update W accordingly.'''\n",
    "            calc_stats_per_batch = False # CHANGE TO TRUE IF YOU WANT TO SEE INTERMEDIATE BATCH STATS\n",
    "\n",
    "            device = utilsNN.set_torch_device()\n",
    "\n",
    "            train_X_batch = train_batched_X[b].to(device = device)\n",
    "            train_Y_batch = train_batched_Y[b].to(device = device)\n",
    "\n",
    "            # print(\"First weight device:\", model.weights[0].device) # TODO: Remove\n",
    "\n",
    "            # Perform forward propagation\n",
    "            forward_results = model.forward(train_X_batch)\n",
    "        \n",
    "            # Calculate the output gradient\n",
    "            output_grad = utilsNN.calc_output_grad(predicted=forward_results, actual=train_Y_batch.to(device=device))\n",
    "        \n",
    "            # Propagate backwards\n",
    "            gradients = model.backward(output_grad=output_grad)\n",
    "            # print(f\"gradients[0] device: {gradients[0].device}\") # TODO: Remove\n",
    "\n",
    "            utilsNN.update_weights(model.weights, gradients, learning_rate)\n",
    "            # Plot training losses versus epochs\n",
    "\n",
    "            # Plot the average training classification error, average testing classification error,\n",
    "            # and value of the loss function after each parameters update (assuming after each batch).\n",
    "            # Also do the batch error for each digit, and overall\n",
    "\n",
    "        # Calculate the stats\n",
    "        device = utilsNN.set_torch_device('cpu') \n",
    "        # Calculate train loss\n",
    "        batch_train_loss = utilsNN.loss_NLL(predicted=forward_results, actual=train_Y_batch)\n",
    "\n",
    "        # Check for nan\n",
    "        if math.isnan(batch_train_loss):\n",
    "            raise Exception(\"ERROR: NAN DETECTED\")\n",
    "        \n",
    "        # Training Accuracy\n",
    "        batch_train_accuracy = utilsNN.calc_acc(forward_results, train_Y_batch)\n",
    "\n",
    "        # Training digit error\n",
    "        batch_train_digit_errors = utilsNN.calc_digit_errors(forward_results, train_Y_batch, 10)\n",
    "        \n",
    "        # Testing stats, need to change the device as well\n",
    "        model.change_weights_device(device)\n",
    "        batch_test_accuracy, batch_test_digit_errors, batch_test_loss = model.pred(test_batched_X[0], test_batched_Y[0])\n",
    "        device = utilsNN.set_torch_device()\n",
    "        model.change_weights_device(device)\n",
    "\n",
    "        # Append the data\n",
    "        ## Train\n",
    "        train_losses.append(batch_train_loss)\n",
    "        train_accuracies.append(batch_train_accuracy)\n",
    "        train_errors_digits.append(batch_train_digit_errors)\n",
    "        ## Test\n",
    "        test_losses.append(batch_test_loss)\n",
    "        test_accuracies.append(batch_test_accuracy)\n",
    "        test_errors_digits.append(batch_test_digit_errors)\n",
    "            \n",
    "        print(f\"Epoch Training | Loss: {batch_train_loss:.4f} | Accuracy: {batch_train_accuracy:.4f}\")\n",
    "\n",
    "        #Take the mean of all the mini-batch losses and denote it as your loss of the current epoch\n",
    "\n",
    "    stats = {\n",
    "        'training' : {\n",
    "            'losses' : train_losses,\n",
    "            'accuracies' : train_accuracies,\n",
    "            'digit errors' : np.transpose(np.array(train_errors_digits))\n",
    "        },\n",
    "        'testing' : {\n",
    "            'losses' : test_losses,\n",
    "            'accuracies' : test_accuracies,\n",
    "            'digit errors' : np.transpose(np.array(test_errors_digits))\n",
    "        },\n",
    "        'epochs' : epochs\n",
    "    }\n",
    "\n",
    "    # Plot the training loss vs accuracy\n",
    "    # Visualize the final weight matrix\n",
    "    # Save the final weight matrix\n",
    "    model.save_weights(\"nn_parameters.txt\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def PA2_test(model = None):\n",
    "    # Specifying the training directory and label files\n",
    "    test_dir = './'\n",
    "    test_anno_file = './data_prog2Spring24/labels/test_anno.csv'\n",
    "    feature_length = 784\n",
    "    # Specifying the device to GPU/CPU. Here, GPU means 'cuda' and CPU means 'cpu'\n",
    "    torch.set_default_device('cpu')\n",
    "    # device = utilsNN.set_torch_device('cpu')\n",
    "\n",
    "    # Get the testing data\n",
    "    # MNIST_testing_dataset = My_dataset(data_dir=test_dir, anno_csv=test_anno_file)\n",
    "    data, Y_encoded = PA2_get_data(test_dir, test_anno_file, 1)\n",
    "    \n",
    "    if (model == None):\n",
    "        # Predict Y using X and updated W.\n",
    "        model = DigitClassifierModelDNN(\n",
    "            input_dim = data.shape[-1],\n",
    "            hidden_layer_num = 2,\n",
    "            hidden_node_num = 100,\n",
    "            output_dim = 10\n",
    "        )\n",
    "\n",
    "    # Load the weights into the model\n",
    "    model.load_weights(\"nn_parameters.txt\")\n",
    "\n",
    "    model.change_weights_device(torch.device('cpu'))\n",
    "\n",
    "    # Make the prediction and get the accuracies\n",
    "    accuracy, digit_errors, loss = model.pred(data[0], Y_encoded[0])\n",
    "\n",
    "    create_conf_mat(model.layers[-1], Y_encoded[0])\n",
    "\n",
    "    # print_tensor(pred, \"pred\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    print(digit_errors)\n",
    "\n",
    "    print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "def line_plot(xvals, xlab : str, ylab: str, yvals0, ylab0 : str, dir : str, yvals1 = None, ylab1 : str = None):\n",
    "    \"\"\"\n",
    "    line_plot Creates a line plot of the xvals vs yvals\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    if yvals1:\n",
    "        plt.plot(xvals, yvals0, label = ylab0)\n",
    "        plt.plot(xvals, yvals1, label = ylab1)\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(xvals, yvals0)\n",
    "    \n",
    "    plt.title(f\"{ylab} vs {xlab}\")\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    file_path = os.path.join(dir, f\"{ylab.replace(' ', '_')}_vs_{xlab.replace(' ', '_')}_plot.png\")\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "def plot_digit_errors(group : str, xvals, xlab, errors, dir : str):\n",
    "    ylab = 'Errors'\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    for i in range(10):\n",
    "        plt.plot(xvals, errors[i], label = str(i))\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.title(f\"{group} Digit {ylab} vs {xlab}\")\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    file_path = os.path.join(dir, f\"{group}_digit_{ylab.replace(' ', '_')}_vs_{xlab.replace(' ', '_')}_plot.png\")\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "def performance_plots(stats : dict):\n",
    "    plot_dir = './plots'\n",
    "    # line_plot(\n",
    "    #     xvals0=stats['epochs'],\n",
    "    #     yvals=stats['training']['losses'],\n",
    "    #     xlabel=\"Epochs\",\n",
    "    #     ylabel=\"Average Training Loss\"\n",
    "    # )\n",
    "    print(stats['training']['losses'])\n",
    "\n",
    "    # Training and testing loss over epochs plot \n",
    "    line_plot(\n",
    "        xvals = stats['epochs'],\n",
    "        xlab='Epochs',\n",
    "        ylab='Average Loss',\n",
    "        yvals0=stats['training']['losses'],\n",
    "        ylab0='Training Losses',\n",
    "        yvals1=stats['testing']['losses'],\n",
    "        ylab1='Testing Losses',\n",
    "        dir = plot_dir\n",
    "    )\n",
    "\n",
    "    # Training and testing accuracies over epochs plot\n",
    "    line_plot(\n",
    "        xvals = stats['epochs'],\n",
    "        xlab='Epochs',\n",
    "        ylab='Average Accuracy',\n",
    "        yvals0=stats['training']['accuracies'],\n",
    "        ylab0='Training Accuracies',\n",
    "        yvals1=stats['testing']['accuracies'],\n",
    "        ylab1='Testing Accuracies',\n",
    "        dir = plot_dir\n",
    "    )\n",
    "\n",
    "    # Training and testing digit errors\n",
    "    plot_digit_errors(\n",
    "        group=\"Training\",\n",
    "        xvals = stats['epochs'],\n",
    "        xlab='Epochs',\n",
    "        errors=stats['training']['digit errors'],\n",
    "        dir = plot_dir\n",
    "    )\n",
    "\n",
    "    plot_digit_errors(\n",
    "        group=\"Testing\",\n",
    "        xvals = stats['epochs'],\n",
    "        xlab='Epochs',\n",
    "        errors=stats['testing']['digit errors'],\n",
    "        dir = plot_dir\n",
    "    )\n",
    "\n",
    "def one_hot_decode(tensor):\n",
    "    return torch.argmax(tensor, dim=1).numpy()\n",
    "\n",
    "def create_conf_mat(prediction, actual):\n",
    "    my_pred = one_hot_decode(prediction)\n",
    "    my_act = one_hot_decode(actual)\n",
    "\n",
    "    cm_fig = plt.figure()\n",
    "    confusion_matrix = metrics.confusion_matrix(y_pred=my_pred, y_true=my_act)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix,\n",
    "                                                display_labels=range(10))\n",
    "    # cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "    cm_display.plot()\n",
    "    plt.savefig('./plots/class_confusion_matrix.jpg')\n",
    "\n",
    "def print_tensor(tensor, tensor_name):\n",
    "    torch.set_printoptions(threshold=1_000)\n",
    "    print(f\"{tensor_name} with shape {tensor.shape}: {tensor}\")\n",
    "    torch.set_printoptions(profile=\"default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "# Specifying the training directory and label files\n",
    "train_dir = './'\n",
    "train_anno_file = './data_prog2Spring24/labels/train_anno.csv'\n",
    "# Specifying the training directory and label files\n",
    "test_dir = './'\n",
    "test_anno_file = './data_prog2Spring24/labels/test_anno.csv'\n",
    "total_batch = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batched_X, train_batched_Y = PA2_get_data(\n",
    "    data_dir=train_dir,\n",
    "    anno_csv=train_anno_file,\n",
    "    batch_num=total_batch\n",
    ")\n",
    "\n",
    "test_batched_X, test_batched_Y = PA2_get_data(\n",
    "    data_dir=train_dir,\n",
    "    anno_csv=train_anno_file,\n",
    "    batch_num=total_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train off of the batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = PA2_train(\n",
    "    train_batched_X=train_batched_X,\n",
    "    train_batched_Y=train_batched_Y,\n",
    "    test_batched_X=test_batched_X,\n",
    "    test_batched_Y=test_batched_Y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_plots(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA2_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
